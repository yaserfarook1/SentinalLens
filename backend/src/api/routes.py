"""
FastAPI Routes - SentinelLens REST API

Endpoints:
- GET  /workspaces - List accessible workspaces
- POST /audits - Start new audit job
- GET  /audits/{job_id} - Get audit job status
- GET  /audits/{job_id}/stream - SSE stream of progress
- GET  /audits/{job_id}/report - Get full report
- POST /audits/{job_id}/approve - Approve tier changes
- GET  /audits - List all audit jobs
- GET  /health - Health check
"""

import logging
import json
import os
from typing import List, Optional
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse

from src.config import settings
from src.models.schemas import (
    StartAuditRequest, ApprovalRequest, WorkspaceInfo, AuditJobMetadata,
    Report, HealthResponse, ErrorResponse, JobStatus,
    TierType, ConfidenceLevel, ReportSummary, TableRecommendation,
    ConnectorCoverageItem, ReportWarning, ExecutionMetadata,
    SetupCredentialsRequest
)
from src.api.auth import validate_entra_token, require_approval_group, extract_user_info
from src.services.azure_api import azure_api_service
from src.security import pii_masking, prompt_shield
from src.security_middleware import security_middleware
from src.utils.logging import AuditLogger

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1", tags=["audit"])


# ===== HEALTH CHECK =====
@router.get(
    "/health",
    response_model=HealthResponse,
    tags=["system"]
)
async def health_check():
    """Health check endpoint (no authentication required)"""
    return HealthResponse(
        status="healthy",
        version=settings.APP_VERSION,
        environment=settings.ENVIRONMENT,
        timestamp=datetime.utcnow()
    )


# ===== SETUP & CONFIGURATION =====
@router.post(
    "/setup/credentials",
    summary="Setup app registration credentials",
    description="Configure Azure AD app registration credentials and save to .env.local",
    tags=["setup"]
)
async def setup_credentials(request: SetupCredentialsRequest):
    """
    Setup app registration credentials.

    This endpoint saves the Azure AD app registration credentials to .env.local
    for local development. In production, these should be stored in Azure Key Vault.

    IMPORTANT: This endpoint has NO authentication check (for initial setup only).
    In production, this should be removed or protected.
    """
    try:
        logger.info("[AUDIT] Credential setup requested")

        # Get the backend directory
        backend_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        env_local_path = os.path.join(backend_dir, ".env.local")

        # Read existing .env.local if it exists
        env_content = {}
        if os.path.exists(env_local_path):
            logger.info(f"[AUDIT] Reading existing .env.local from {env_local_path}")
            with open(env_local_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#") and "=" in line:
                        key, value = line.split("=", 1)
                        env_content[key.strip()] = value.strip()

        # Update with new credentials
        env_content["AZURE_CLIENT_ID"] = request.client_id
        env_content["AZURE_CLIENT_SECRET"] = request.client_secret

        # Write back to .env.local
        with open(env_local_path, "w") as f:
            f.write("# Backend Configuration\n")
            f.write("# This file was auto-generated by the setup endpoint\n")
            f.write("# IMPORTANT: Never commit this to Git!\n\n")
            for key, value in env_content.items():
                f.write(f"{key}={value}\n")

        logger.info(f"[AUDIT] Credentials saved to {env_local_path}")

        return {
            "status": "success",
            "message": "Credentials configured successfully",
            "env_file": env_local_path,
            "note": "Please restart the backend for changes to take effect",
            "client_id": request.client_id[:20] + "..." if len(request.client_id) > 20 else request.client_id
        }

    except Exception as e:
        logger.error(f"[AUDIT] Credential setup failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to setup credentials: {str(e)}"
        )


# ===== WORKSPACE MANAGEMENT =====
@router.get(
    "/workspaces",
    response_model=List[WorkspaceInfo],
    summary="List accessible Sentinel workspaces",
    description="Returns all Sentinel workspaces accessible to the authenticated user"
)
async def get_workspaces(token: dict = Depends(validate_entra_token)):
    """
    List accessible Sentinel workspaces.

    Returns workspaces from subscriptions the user has access to.
    """
    try:
        user_info = extract_user_info(token)
        logger.info(f"[AUDIT] Workspace list requested by: {user_info['user_principal']}")

        # Get list of workspaces from Azure
        try:
            azure_workspaces = await azure_api_service.list_workspaces()
        except Exception as azure_err:
            logger.error(f"[AUDIT] Azure API error: {type(azure_err).__name__}: {str(azure_err)}", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Azure API error: {str(azure_err)}"
            )

        # Convert to WorkspaceInfo format
        workspaces = [
            WorkspaceInfo(
                workspace_id=ws["workspace_id"],
                workspace_name=ws["workspace_name"],
                subscription_id=ws["subscription_id"],
                resource_group=ws.get("resource_group", "unknown")
            )
            for ws in azure_workspaces
        ]

        logger.info(f"[AUDIT] Returned {len(workspaces)} workspaces")
        return workspaces

    except HTTPException:
        raise

    except Exception as e:
        logger.error(f"[AUDIT] Workspace list failed: {type(e).__name__}: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Workspace list failed: {str(e)}"
        )


# ===== AUDIT JOB MANAGEMENT =====
@router.post(
    "/audits",
    response_model=AuditJobMetadata,
    summary="Start a new audit job",
    description="Initiate cost optimization audit for a Sentinel workspace"
)
async def start_audit(
    request: StartAuditRequest,
    token: dict = Depends(validate_entra_token)
):
    """
    Start a new audit job.

    Validates Entra token, creates audit job, and returns job ID.
    Async job runs in background - client polls /audits/{job_id} for status.
    """
    try:
        user_info = extract_user_info(token)

        # Validate prompt shield on user inputs
        is_safe, risk_score, reason = prompt_shield.validate(request.workspace_id)
        if not is_safe:
            logger.warning(
                f"[AUDIT] Prompt injection detected in audit request: {reason}"
            )
            # Log security event
            security_middleware.log_security_event(
                event_type="PROMPT_INJECTION_ATTEMPT",
                severity="HIGH",
                details=f"Injection detected in workspace_id: {reason}",
                user_id=user_info['user_id']
            )
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid input detected"
            )

        logger.info(
            f"[AUDIT] Audit job started by: {user_info['user_principal']} "
            f"for workspace: {request.workspace_id}"
        )

        # Log audit event
        AuditLogger.log_event(
            event_type="AUDIT_STARTED",
            resource=request.workspace_id,
            status="SUCCESS",
            user_id=user_info['user_id'],
            details=f"days_lookback={request.days_lookback}"
        )

        # Create job (in production, would create database record)
        job_id = f"job-{datetime.utcnow().timestamp()}"

        # Return job metadata
        return AuditJobMetadata(
            job_id=job_id,
            workspace_id=request.workspace_id,
            status=JobStatus.QUEUED,
            created_at=datetime.utcnow(),
            error_message=None,
            report_url=None
        )

    except HTTPException:
        raise

    except Exception as e:
        logger.error(f"[AUDIT] Audit start failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to start audit job"
        )


@router.get(
    "/audits/{job_id}",
    response_model=AuditJobMetadata,
    summary="Get audit job status",
    description="Retrieve status and metadata for an audit job"
)
async def get_audit_status(
    job_id: str,
    token: dict = Depends(validate_entra_token)
):
    """Get audit job status and metadata"""
    try:
        user_info = extract_user_info(token)
        logger.info(f"[AUDIT] Status requested for job: {job_id}")

        # Placeholder: In production, would query database
        return AuditJobMetadata(
            job_id=job_id,
            workspace_id="placeholder-ws",
            status=JobStatus.RUNNING,
            created_at=datetime.utcnow(),
            error_message=None,
            report_url=None
        )

    except Exception as e:
        logger.error(f"[AUDIT] Status check failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to get job status"
        )


@router.get(
    "/audits/{job_id}/stream",
    summary="Real-time audit progress (SSE)",
    description="Server-sent events stream of agent tool execution progress"
)
async def stream_audit_progress(
    job_id: str,
    token: dict = Depends(validate_entra_token)
):
    """
    Stream audit progress via Server-Sent Events (SSE).

    Returns real-time updates as agent executes tools and completes steps.
    """
    async def event_generator():
        try:
            logger.info(f"[AUDIT] SSE stream opened for job: {job_id}")

            # Placeholder: In production, would stream events from agent execution
            # NOTE: SSE data MUST be valid JSON (use json.dumps for proper formatting)
            yield f"data: {json.dumps({'job_id': job_id, 'current_step': 1, 'total_steps': 3, 'current_tool': 'list_tables', 'tool_description': 'Listing all tables in workspace', 'status': 'Running', 'progress_percent': 33})}\n\n"
            yield f"data: {json.dumps({'job_id': job_id, 'current_step': 2, 'total_steps': 3, 'current_tool': 'get_ingestion_volume', 'tool_description': 'Calculating ingestion volumes', 'status': 'Running', 'progress_percent': 66})}\n\n"
            yield f"data: {json.dumps({'job_id': job_id, 'current_step': 3, 'total_steps': 3, 'current_tool': 'list_analytics_rules', 'tool_description': 'Analyzing analytics rules', 'status': 'Completed', 'progress_percent': 100})}\n\n"

        except Exception as e:
            logger.error(f"[AUDIT] SSE stream error: {str(e)}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@router.get(
    "/audits/{job_id}/report",
    response_model=Report,
    summary="Get full optimization report",
    description="Retrieve complete cost optimization analysis and recommendations"
)
async def get_report(
    job_id: str,
    token: dict = Depends(validate_entra_token)
):
    """Get full audit report"""
    try:
        user_info = extract_user_info(token)
        logger.info(f"[AUDIT] Report requested for job: {job_id}")

        # Generate mock report for demo purposes
        # In production, would fetch from Blob Storage
        mock_report = Report(
            job_id=job_id,
            workspace_id="/subscriptions/b8f99f9f-c121-422b-a657-c999df2c5296/resourceGroups/rg-jayesh/providers/Microsoft.OperationalInsights/workspaces/PCS-Sentinel-Demo",
            workspace_name="PCS-Sentinel-Demo",
            timestamp=datetime.now(),
            summary=ReportSummary(
                total_tables_analyzed=120,
                total_ingestion_gb_per_month=450.0,
                total_monthly_cost_hot=15000.0,
                total_monthly_cost_archive=1500.0,
                total_monthly_savings=13500.0,
                total_annual_savings=162000.0,
                tables_by_tier={"Hot": 85, "Basic": 25, "Archive": 10},
                tables_by_confidence={"HIGH": 45, "MEDIUM": 55, "LOW": 20}
            ),
            archive_candidates=[
                TableRecommendation(
                    table_name="OldSecurityEvent",
                    current_tier=TierType.HOT,
                    ingestion_gb_per_day=0.1,
                    ingestion_gb_per_month=3.0,
                    rule_coverage_count=0,
                    rule_names=[],
                    confidence=ConfidenceLevel.HIGH,
                    parsing_confidence=1.0,
                    monthly_cost_hot=100.0,
                    monthly_cost_archive=10.0,
                    monthly_savings=90.0,
                    annual_savings=1080.0,
                    notes="No analytics rules reference this table"
                ),
                TableRecommendation(
                    table_name="UnusedEvents",
                    current_tier=TierType.HOT,
                    ingestion_gb_per_day=0.05,
                    ingestion_gb_per_month=1.5,
                    rule_coverage_count=0,
                    rule_names=[],
                    confidence=ConfidenceLevel.HIGH,
                    parsing_confidence=1.0,
                    monthly_cost_hot=50.0,
                    monthly_cost_archive=5.0,
                    monthly_savings=45.0,
                    annual_savings=540.0,
                    notes="Ingestion stopped 60 days ago"
                )
            ],
            low_usage_candidates=[
                TableRecommendation(
                    table_name="RareAuditLog",
                    current_tier=TierType.HOT,
                    ingestion_gb_per_day=0.02,
                    ingestion_gb_per_month=0.6,
                    rule_coverage_count=1,
                    rule_names=["Rare Compliance Check"],
                    confidence=ConfidenceLevel.MEDIUM,
                    parsing_confidence=0.85,
                    monthly_cost_hot=20.0,
                    monthly_cost_archive=2.0,
                    monthly_savings=18.0,
                    annual_savings=216.0,
                    notes="Only 1 low-priority rule references this"
                )
            ],
            active_tables=[
                TableRecommendation(
                    table_name="SecurityEvent",
                    current_tier=TierType.HOT,
                    ingestion_gb_per_day=5.0,
                    ingestion_gb_per_month=150.0,
                    rule_coverage_count=45,
                    rule_names=["Detection Rule 1", "Detection Rule 2", "Detection Rule 3"],
                    confidence=ConfidenceLevel.HIGH,
                    parsing_confidence=1.0,
                    monthly_cost_hot=5000.0,
                    monthly_cost_archive=500.0,
                    monthly_savings=4500.0,
                    annual_savings=54000.0,
                    notes="Critical table - 45 active detection rules depend on this"
                ),
                TableRecommendation(
                    table_name="SigninLogs",
                    current_tier=TierType.HOT,
                    ingestion_gb_per_day=2.0,
                    ingestion_gb_per_month=60.0,
                    rule_coverage_count=12,
                    rule_names=["Suspicious Signin Rule 1", "Suspicious Signin Rule 2"],
                    confidence=ConfidenceLevel.HIGH,
                    parsing_confidence=1.0,
                    monthly_cost_hot=2000.0,
                    monthly_cost_archive=200.0,
                    monthly_savings=1800.0,
                    annual_savings=21600.0,
                    notes="Used in 12 detection rules and 3 workbooks"
                )
            ],
            connector_coverage=[
                ConnectorCoverageItem(
                    connector_name="Azure Security Center",
                    connector_type="Scheduled Query",
                    tables_fed=["SecurityEvent", "SecurityAlert"],
                    tables_with_coverage=2,
                    tables_without_coverage=0
                ),
                ConnectorCoverageItem(
                    connector_name="Azure Activity",
                    connector_type="Data Connector",
                    tables_fed=["AzureActivity"],
                    tables_with_coverage=1,
                    tables_without_coverage=0
                )
            ],
            warnings=[
                ReportWarning(
                    warning_type="LOW_CONFIDENCE",
                    table_name="ComplexKQLTable",
                    description="KQL parsing had low confidence for rules using this table",
                    recommendation="Manually review the 3 rules referencing this table"
                ),
                ReportWarning(
                    warning_type="EXTERNAL_DEPENDENCY",
                    table_name="CustomTable",
                    description="This table feeds an external reporting system",
                    recommendation="Consult with reporting team before archiving"
                )
            ],
            metadata=ExecutionMetadata(
                agent_run_timestamp=datetime.now(),
                agent_completion_time_seconds=45.3,
                kql_parsing_success_rate=0.92,
                tables_analyzed=120,
                rules_analyzed=156,
                workbooks_analyzed=23,
                hunt_queries_analyzed=45,
                agent_tokens_used=8500,
                agent_tokens_limit=50000
            )
        )

        logger.info(f"[AUDIT] Report generated successfully for job: {job_id}")
        return mock_report

    except HTTPException:
        raise

    except Exception as e:
        logger.error(f"[AUDIT] Report fetch failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to get report"
        )


# ===== APPROVAL GATE (HARD SEPARATION) =====
@router.post(
    "/audits/{job_id}/approve",
    summary="Approve tier changes (HARD GATE)",
    description="Execute table tier migrations - requires security group membership"
)
async def approve_migration(
    job_id: str,
    request: ApprovalRequest,
    token: dict = Depends(validate_entra_token),
    authorized: bool = Depends(require_approval_group)
):
    """
    Approve and execute tier changes.

    HARD GATE: This is a separate service path with its own authentication.
    Only users in approval security group can execute tier changes.
    All approvals logged immutably.

    Args:
        job_id: Audit job ID
        request: List of tables to migrate
        token: Validated Entra ID token
        authorized: Must be member of approval group

    Returns:
        Approval result and execution status
    """
    try:
        if not authorized:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Not authorized"
            )

        user_info = extract_user_info(token)

        logger.warning(
            f"[AUDIT] TIER CHANGE APPROVED AND EXECUTED by: {user_info['user_principal']} "
            f"job_id={job_id} tables={len(request.table_names)}"
        )

        # Placeholder: In production, would execute tier changes via API
        return {
            "status": "approved",
            "job_id": job_id,
            "tables_migrated": request.table_names,
            "executed_at": datetime.utcnow()
        }

    except HTTPException:
        raise

    except Exception as e:
        logger.error(f"[AUDIT] Approval failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Approval execution failed"
        )


# ===== AUDIT HISTORY =====
@router.get(
    "/audits",
    response_model=List[AuditJobMetadata],
    summary="List all audit jobs",
    description="Retrieve history of audit jobs with pagination"
)
async def list_audits(
    skip: int = 0,
    limit: int = 50,
    token: dict = Depends(validate_entra_token)
):
    """List all audit jobs (paginated)"""
    try:
        user_info = extract_user_info(token)
        logger.info(f"[AUDIT] Audit history requested")

        # Placeholder: In production, would query database with pagination
        return []

    except Exception as e:
        logger.error(f"[AUDIT] Audit list failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to list audits"
        )


# ===== ERROR HANDLING =====
@router.get(
    "/error",
    response_model=ErrorResponse,
    tags=["system"]
)
async def error_example():
    """Example error response"""
    return ErrorResponse(
        error_code="EXAMPLE",
        error_message="This is an example error",
        timestamp=datetime.utcnow()
    )
